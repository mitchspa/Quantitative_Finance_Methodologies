{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdddeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c2690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import poisson\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import datetime as dt\n",
    "import yfinance as yfin\n",
    "\n",
    "from mpmath import mp\n",
    "mp.dps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a607a",
   "metadata": {},
   "source": [
    "# Multiple Parameters Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ac9f5",
   "metadata": {},
   "source": [
    "When dealing with multiple parameters of non-smooth functional data, it's well-known that multiple local minima may exist. To address this issue, two main solutions are commonly employed, to the best of my knowledge:\n",
    "\n",
    "> Bayesian Methods: Constructing a Markov Chain Monte Carlo (MCMC) chain is a feasible approach, especially when a dynamics model for the underlying asset is available. This dynamics, expressed as a distribution of log-returns, serves as the Likelihood function. You can specify a prior based on your intuition or a frequentist estimate of the mean and variance, though the latter approach implies the exploitation of data twice.\n",
    "\n",
    "Here's a simplified example of MCMC to calibrate a Geometric Brownian Motion (GBM) model (toy problem):\n",
    "$$X_{t}=\\bigg(r-\\frac{\\sigma^{2}}{2}\\bigg)t + \\sigma W_{t}\\to X_{t}\\sim N\\bigg(\\bigg(r-\\frac{\\sigma^{2}}{2}\\bigg)t,\\sigma^{2}t\\bigg).$$\n",
    "\n",
    "$$\\text{Prior } \\sigma\\sim \\pi(\\sigma),\\hspace{1cm} \\text{Likelihood } X_{t}|\\sigma\\sim N\\bigg(\\bigg(r-\\frac{\\sigma^{2}}{2}\\bigg)t,\\sigma^{2}t\\bigg)=:L(X_{t}|\\sigma).$$\n",
    "\n",
    "$$\\text{By Bayes, }\\pi(\\sigma|X_{t})\\propto L(X_{t}|\\sigma)\\times \\pi(\\sigma)=:Kernel(\\sigma).$$\n",
    "\n",
    "- Initialize $\\sigma^{(0)}$.\n",
    "- For i in range(N):\n",
    "\n",
    "    - Propose a candidate $\\sigma^{(i),c}=\\sigma^{(i-1)}+\\text{ perturbation }$.\n",
    "    \n",
    "    - Draw from a uniform distribution $U(0,1)$ the sample u.\n",
    "    \n",
    "    - If $$\\frac{Kernel(\\sigma^{(i),c})}{Kernel(\\sigma^{(i-1)})}>u\\hspace{1cm} \\sigma^{(i)}=\\sigma^{(i),c},$$\n",
    "    otherwise $$\\sigma^{(i)}=\\sigma^{(i-1)}.$$\n",
    "\n",
    "- The chain $\\{\\sigma^{(burn-in)},...,\\sigma^{(N)}\\}$ represents $N-(burn-in-ratio)$ samples from the posterior of $\\sigma$ for N large enough. Adopt the mean of those samples as an estimation of the parameter.\n",
    "\n",
    "> Gradient-based Optimization: Algorithms like ADAM and similar methods are frequently employed. However, it's essential to deeply investigate your cost function and ensure adaptivity of the learning rate to prevent being trapped in local minima.\n",
    "\n",
    "$\\textbf{HOWEVER,}$ here I propose a convergent but inefficient alternative: a brute-force raw algorithm to calibrate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25662e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization setup\n",
    "iterations = 10**4 #higher is better\n",
    "step = 0.02 #lower is better with higher number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83aeec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(f, T, data):\n",
    "    \n",
    "    sig = signature(f)\n",
    "    n_params = len(sig.parameters) -  1\n",
    "    \n",
    "    init = np.zeros(n_params)\n",
    "    chain = [init]\n",
    "    errors =  10**6\n",
    "\n",
    "    choices = np.random.choice([1, 0], size=(iterations, n_params))\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        temp = np.abs(chain[-1] + choices[i] *  step)\n",
    "        error = np.average((f(T, *temp) - data)**2)\n",
    "        \n",
    "        if error < errors:\n",
    "            chain.append(temp)\n",
    "            errors = error\n",
    "            \n",
    "    return chain[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c91898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock:\n",
    "    def __init__(self, T, data, N, rf,S0):\n",
    "        self.T = T\n",
    "        interp = scipy.interpolate.CubicSpline(self.T[0::10],data[0::10])\n",
    "        self.data = interp(self.T)\n",
    "        self.N = N\n",
    "        self.rf = rf\n",
    "        self.S0 = S0\n",
    "        self.M = T.shape[0]\n",
    "        \n",
    "    def underlying_simulation(self, T, params, model):\n",
    "        X = np.zeros((self.N, self.M))\n",
    "        dt = self.T[1]-self.T[0]\n",
    "\n",
    "        if model == 'gbm':\n",
    "            flag =  1\n",
    "\n",
    "            sigma = np.array(params)\n",
    "            corr = +0.5*(sigma**2)\n",
    "\n",
    "        elif model == 'merton':\n",
    "            flag =  2\n",
    "            sigma, lambda_, muJ, sigmaJ = params\n",
    "            corr = +0.5*(sigma**2) + lambda_ * (np.exp(0.5*sigmaJ**2 +  muJ) -  1)\n",
    "\n",
    "        elif model == 'kou':\n",
    "            flag =  3\n",
    "            sigma, lambda_, p, lamp, lamm = params\n",
    "            corr = +0.5*(sigma**2) +  lambda_ * (p / (lamp -1) - (1 - p) / (lamm +  1))\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Error in model specification. Use gbm, merton or kou.')\n",
    "        \n",
    "        drift = rf-corr\n",
    "\n",
    "        Z = np.random.randn(self.N, self.M)\n",
    "\n",
    "        if flag ==  1:\n",
    "            for i in range(self.M-1):\n",
    "                X[:, i+1] = X[:, i] + np.ones(self.N)*drift * dt + np.sqrt(dt) * sigma * Z[:, i]\n",
    "\n",
    "        elif flag ==  2:\n",
    "            Ndt = poisson.rvs(lambda_ * dt, size=(self.M,self.N))\n",
    "            for i in range(self.M-1):\n",
    "                X[:, i+1] = X[:, i] + np.ones(self.N)*drift * dt + np.sqrt(dt) * sigma * Z[:, i]\n",
    "                for j in range(self.N):\n",
    "                    if Ndt[i,j] >  0:\n",
    "                        Y = sum(muJ + sigmaJ * np.random.normal(0, 1, Ndt[i,j]))\n",
    "                        X[j, i+1] = X[j, i+1] + Y\n",
    "\n",
    "        elif flag ==  3:\n",
    "            Ndt = poisson.rvs(lambda_ * dt, size=(self.M,self.N))\n",
    "            for i in range(self.M-1):\n",
    "                X[:, i+1] = X[:, i] + np.ones(self.N)*drift * dt + np.sqrt(dt) * sigma * Z[:, i]\n",
    "                for j in range(self.N):\n",
    "                    for jj in range(Ndt[i,j]):\n",
    "                        if np.random.uniform(0,1,1) < p:  # positive\n",
    "                            Y = np.random.exponential(lamp)\n",
    "                        else:\n",
    "                            Y = -np.random.exponential(lamm)\n",
    "                        X[j, i+1] = X[j, i+1] + Y\n",
    "\n",
    "        S = self.S0 * np.exp(np.average(X,axis=0))\n",
    "        return S\n",
    "    \n",
    "    \n",
    "    def calibrate(self, model):\n",
    "        if model == 'gbm':\n",
    "            def wrapper(T, sigma):\n",
    "                return self.underlying_simulation(T, [sigma], model)\n",
    "        if model == 'merton':\n",
    "            def wrapper(T, sigma, lambda_, muJ, sigmaJ):\n",
    "                par = [sigma, lambda_, muJ, sigmaJ]\n",
    "                return self.underlying_simulation(T, par, model)\n",
    "        if model == 'kou':\n",
    "            def wrapper(T, sigma, lambda_, p, lamp, lamm):\n",
    "                par = [sigma, lambda_, p, lamp, lamm]\n",
    "                return self.underlying_simulation(T, par, model)\n",
    "\n",
    "        popt = optimizer(wrapper, self.T, self.data)\n",
    "\n",
    "        return popt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181ec4",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7abe5c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[0.28 0.26 0.1  0.24 0.22]\n"
     ]
    }
   ],
   "source": [
    "# model specifications\n",
    "model = 'kou'\n",
    "rf = 0.05 # to be observed\n",
    "\n",
    "# data\n",
    "yfin.pdr_override()\n",
    "aapl = np.array(pdr.get_data_yahoo('AAPL', start='2023-08-21', end='2024-02-21')['Close'])\n",
    "T = np.linspace(0,0.5,aapl.shape[0])\n",
    "\n",
    "# calibration\n",
    "asset = stock(T, aapl, 1000, rf, aapl[0])\n",
    "par = asset.calibrate(model)\n",
    "\n",
    "print(par)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
